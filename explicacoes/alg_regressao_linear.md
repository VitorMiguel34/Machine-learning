# Algoritmo de RegressƒÅo Linear - Explica√ßƒÅo
---

## O que √© Regress√£o Linear?

A regress√£o linear √© um modelo que busca descrever a rela√ß√£o entre um resultado e suas vari√°veis, ajustando uma reta (ou hiperplano) aos dados.

Em sua forma mais simples (1 vari√°vel):

\[
y = wx + b
\]

Onde:
- `x` ‚Üí vari√°vel de entrada (feature)
- `y` ‚Üí valor previsto
- `a` ‚Üí peso (inclina√ß√£o da reta)
- `b` ‚Üí bias (intercepto)

O objetivo do modelo √© encontrar os valores de `a` e `b` que melhor se ajustam aos dados.

## Interpreta√ß√£o geom√©trica

Do ponto de vista geom√©trico, cada valor √© um ponto. O algoritmo de regress√£o linear tenta
achar a reta que fique o mais pr√≥xima poss√≠vel desses pontos. Em caso de mais de uma vari√°vel,
o algoritmo busca o hiperplano que mais se aproxime desses pontos.

---

## üìâ Fun√ß√£o de erro (MSE)

Para saber se uma reta √© boa, precisamos medir o erro.
No modelo de regress√£o linear, esse erro corresponde √† diferen√ßa entre o valor real e o valor previsto pelo modelo.
O erro quadr√°tico simplesmente pega o erro e o eleva ao quadrado. Logo, conclu√≠mos que, para maximizar o desempenho do modelo, precisamos minimizar a fun√ß√£o que indica o erro.

A fun√ß√£o usada √© a **Soma dos Erros Quadr√°ticos**, que √© a soma dos erros quadr√°ticos de todos os valores, descrita pela seguinte fun√ß√£o:

$$
SSE = \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2
$$

ou

$$
SSE = \sum_{i=1}^{n} \left( y_i - (a x_i + b) \right)^2
$$

Onde:

$$
y_i \quad \text{valor real}
$$

$$
\hat{y}_i \quad \text{valor previsto pelo modelo}
$$

### Por que erro ao quadrado?
Utilizamos o erro quadr√°tico na fun√ß√£o porque ele penaliza erros grandes, dando uma boa estimativa do desempenho do modelo.

---

## Como minimizar essa fun√ß√£o?

Para minimizar a Soma dos Erros Quadr√°ticos (SSE), utilizamos **derivadas parciais** para identificar o ponto em que a fun√ß√£o de erro atinge seu valor m√≠nimo.
Como a fun√ß√£o de erro depende dos par√¢metros do modelo (`a` e `b`), precisamos analisar como o erro varia em rela√ß√£o a cada um deles separadamente.

Como o erro depende de mais de uma vari√°vel (`a` e `b`), usamos **derivadas parciais** para medir:

- Como o erro muda quando variamos `a` (mantendo `b` fixo)
- Como o erro muda quando variamos `b` (mantendo `a` fixo)

Essas derivadas indicam a dire√ß√£o de maior crescimento do erro.  
Para minimizar, seguimos a dire√ß√£o oposta.

---

## Derivadas parciais

Derivada parcial em rela√ß√£o a `a`:

$$
\frac{\partial \sum_{i=1}^{n} (y_i - (a x_i + b))^2}{\partial a}
$$

Derivada parcial em rela√ß√£o a `b`:

$$
\frac{\partial \sum_{i=1}^{n} (y_i - (a x_i + b))^2}{\partial b}
$$

Em um modelo com n vari√°veis, teremos n + 1 par√¢metros que, ao calcular as derivadas parciais da soma dos erros quadr√°ticos em rela√ß√£o a cada um deles, formam um sistema (n + 1) √ó (n + 1) para resolver.
Ao resolver o sistema, encontraremos o valor dos par√¢metros que minimizam a soma dos erros quadr√°ticos.

**OBS**:Como o modelo de regressƒÅo linear prev√™ resultados de acordo com uma reta, sempre podemos encontrar uma reta mais distante dos valores dados em treinamento, ou seja, sempre podemos encontrar uma reta cuja soma dos erros quadr√°ticos seja maior que a reta anterior, o que significa que as derivadas parciais indicam a reta que minimiza a SSE, nƒÅo a que maximiza
