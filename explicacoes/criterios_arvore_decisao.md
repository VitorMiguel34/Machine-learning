# üìä Crit√©rios de Impureza em √Årvores de Decis√£o  
### Gini, Entropia e Log Loss

Em modelos de **√°rvores de decis√£o para classifica√ß√£o**, os crit√©rios de impureza s√£o usados para decidir **como dividir os dados em cada n√≥**. O objetivo √© sempre criar subconjuntos cada vez mais ‚Äúpuros‚Äù, ou seja, contendo predominantemente exemplos de uma √∫nica classe.

Neste arquivo, s√£o explorados tr√™s crit√©rios principais:
- **Gini**
- **Entropia**
- **Log Loss**

---

## üå≥ O que √© Impureza?

A **impureza** mede o qu√£o misturadas est√£o as classes em um n√≥ da √°rvore.

- N√≥ **puro** ‚Üí todas as amostras pertencem √† mesma classe  
- N√≥ **impuro** ‚Üí v√°rias classes misturadas  

Quanto **menor a impureza**, melhor √© o n√≥.

---

## üîπ Gini (√çndice de Gini)

O **√çndice de Gini** quantifica o qu√£o misturado est√° um n√≥ em termos de classes, medindo a **probabilidade de erro ao rotular uma amostra de forma aleat√≥ria**, usando a pr√≥pria distribui√ß√£o de classes presente no n√≥.

A intui√ß√£o √© a seguinte: imagine que voc√™ escolhe **uma amostra aleat√≥ria de um n√≥** e tenta prever sua classe **sorteando uma classe de acordo com as propor√ß√µes existentes nesse n√≥**. O valor do Gini representa a **probabilidade de essa previs√£o estar errada**.

Dessa forma:
- Se o n√≥ √© composto quase inteiramente por uma √∫nica classe, a chance de erro √© pequena ‚Üí **Gini baixo**
- Se o n√≥ possui classes bem equilibradas, a chance de erro √© alta ‚Üí **Gini alto**

### F√≥rmula

$$
Gini = 1 - \sum_{i=1}^{n} p_i^2
$$

onde:
- $p_i$ √© a propor√ß√£o da classe *i* no n√≥  
- $n$ √© o n√∫mero de classes

### Propriedades

- Varia de **0 a aproximadamente 0.5** (em classifica√ß√£o bin√°ria)
- **0** indica um n√≥ totalmente puro

### Interpreta√ß√£o

- Favorece divis√µes que isolam rapidamente a classe majorit√°ria

---

## üîπ Entropia

A **Entropia**, originada da Teoria da Informa√ß√£o, mede o **n√≠vel de incerteza** associado √† classe de uma amostra escolhida aleatoriamente em um n√≥ da √°rvore.

Ela responde, de forma intuitiva, √† pergunta:  
**‚ÄúQuanta informa√ß√£o √© necess√°ria, em m√©dia, para identificar corretamente a classe de uma amostra desse n√≥?‚Äù**

Se o n√≥ √© quase puro, a incerteza √© baixa e pouca informa√ß√£o √© necess√°ria.  
Se as classes est√£o bem distribu√≠das, a incerteza √© alta e mais informa√ß√£o √© necess√°ria.

Assim, a entropia cresce √† medida que as classes ficam mais misturadas e imprevis√≠veis.

### F√≥rmula

$$
Entropia = - \sum_{i=1}^{n} p_i \log_2(p_i)
$$

### Propriedades

- Varia de **0 a $\log_2(n)$**, onde *n* √© o n√∫mero de classes
- **0** indica um n√≥ puro
- Penaliza mais fortemente divis√µes muito incertas

### Interpreta√ß√£o

- Quanto maior a entropia, maior a desordem
- Busca divis√µes que maximizem o **ganho de informa√ß√£o**

---

## üîπ Log Loss (Logarithmic Loss)

O **Log Loss** avalia a qualidade das **probabilidades previstas pelo modelo**, levando em conta n√£o apenas se a classe prevista est√° correta, mas tamb√©m **o n√≠vel de confian√ßa associado a essa previs√£o**.

Ele pode ser interpretado como a resposta √† pergunta:  
**‚ÄúO modelo atribuiu alta probabilidade √† classe correta?‚Äù**

A ideia central √©:
- Previs√µes corretas com **alta confian√ßa** s√£o pouco penalizadas
- Previs√µes erradas com **alta confian√ßa** s√£o severamente penalizadas
- Previs√µes incertas recebem penaliza√ß√µes intermedi√°rias

Por isso, o Log Loss √© especialmente √∫til para modelos probabil√≠sticos, nos quais errar ‚Äúcom muita certeza‚Äù √© considerado muito pior do que errar de forma incerta.

### F√≥rmula

$$
LogLoss = - \sum_{i=1}^{n} y_i \log(p_i)
$$

onde:
- $y_i = 1$ se a classe correta for *i*, caso contr√°rio $0$
- $p_i$ √© a probabilidade prevista para a classe *i*

### Propriedades

- Nunca √© negativa
- Quanto menor, melhor
- Extremamente sens√≠vel a erros cometidos com alta confian√ßa

### Interpreta√ß√£o

- Avalia n√£o apenas a classe prevista, mas tamb√©m **o n√≠vel de confian√ßa** do modelo

---

## ‚öñÔ∏è Compara√ß√£o entre os Crit√©rios

| Crit√©rio   | Mede o qu√™? | Vantagens | Observa√ß√µes |
|-----------|------------|----------|-------------|
| **Gini** | Impureza | R√°pido e eficiente | Muito usado na pr√°tica |
| **Entropia** | Incerteza | Base te√≥rica forte | Resultados similares ao Gini |
| **Log Loss** | Qualidade probabil√≠stica | Penaliza erros confiantes | Pode gerar √°rvores mais profundas |

---

## üß† Observa√ß√µes Importantes

- Em muitos casos, **Gini e Entropia produzem √°rvores muito parecidas**
- Um n√≥ pode **n√£o ser puro** e, ainda assim:
  - n√£o se dividir mais
  - prever uma classe com probabilidade 1 (classe majorit√°ria)
- √Årvores de decis√£o **n√£o s√£o bons estimadores de probabilidade**

---

## üìå Conclus√£o

Os crit√©rios de impureza definem como a √°rvore aprende a separar os dados.  
Entender **Gini**, **Entropia** e **Log Loss** √© essencial para interpretar corretamente o comportamento de √°rvores de decis√£o e seus limites, especialmente no que diz respeito a previs√µes probabil√≠sticas.
